---
title: "Cluster and Robust"
author: "Alex"
date: \today
output: pdf_document
---

# Theory 
How should one estimate robust and clustered variance covariance matrices?

We talk about assumptions for regression. These assumptions are the Gauss-Markov set of assumptions, but boil down to:

1. We have written down the right model;
2. The data iid
3. The variance of the data is finite, and constant.

In this little demo, we're really just talking about the last of these.
In practice, especially with regression analysis of experiments, but also
more broadly, we often either don't have strong theoretical reasons to
expect the variance in our residuals (the squared deviations from the
regression line) to be constant at all points in the distribution.
At least, I don't have strong priors for most types of data; which makes
me think that you probably shouldn't either.

Think about what such a strong prior would mean! You would have to:

1. Have a mental model of the data generating process;
2. Have a mental model of the best fitting linear trend about that DGP;
3. Know that there is no difference in the performance of the model at
   different points in the model. That's deep.

Here's the process when we're thinking about Standard Errors, and getting them
right.

1. Estimate your linear model. Whether this is gaussian (OLS), binomial (logit,probit), counting (Poisson), time to failure (exponential), or some other more esoteric form *IT DOESN'T MATTER* -- the estimation of coefficients is a distinct task from getting the uncertainty about those estimates correct.
2. Think about the data generating process. Does the assignment to treatment have any clustering to it? If not, then estimate robust standard errors. If yes, then estimate cluster-robust standard errors. Do the outcomes have some clustering to them? Are there groupings in the data? Then you should probably include a fixed-effect for each group, and also estimate cluster-robust standard errors.
3. The estimation process is straightforward.

- Load Packages
  - For both: load the `lmtest` package for easy testing.
  - For both: load the `sandwich` package for estimating robust SEs
  - For clustering: load the additional package `multiwayvcov`
- Compute Appropriate VCOV
  - robust : `vcovHC`
  - cluster: `cluster.vcov`

# Demos 
```{r, message=FALSE}
library(lmtest)
library(sandwich)
library(multiwayvcov)
library(data.table)
library(stargazer)
``` 

Begin by loading the sample data 

```{r}
rm(list = ls())
data(petersen)
p <- petersen           # who has time to type that a bunch of times? 
rm(petersen)
ls()
``` 

This data is simulated data with 500 firms identified over 10 separate years.

- `firmid`: the firm identifier 
- `year`  : the year, ordered from 1-10
- `x`     : the RHS variable
- `y`     : the LHS variable 

```{r}
head(p)
``` 

## Estimate Coefficients 
Now, we can *really* easily fit a model for this. 

```{r}
m1 <- lm(y ~ x, data = p)
``` 

## Estimate Uncertainty  

Robust standard errors are calculated using the `sandwich` package, and via the `vcovHC` function call, which is the **H**eteroskedastic **C**onsistent **V**ariance **Co****V**ariance estimator. 

```{r}
## ? vcovHC
m1$vcovHC <- vcovHC(m1)
coeftest(m1)
```

```{r, results='asis'}
stargazer(m1, se=list(sqrt(diag(m1$vcovHC))), header=F)
```

Clustered standard errors are not much more difficult. They are, by their nature, not only estimating the quantity of the covariance within the cluster, but are also estimating robust estimates as well. 

```{r}
## 
## ?multiwayvcov::cluster.vcov
## 

## one way clustering 
m1$cluster1.vcov <- cluster.vcov(m1, ~ firmid)
## two way clustering
m1$cluster2.vcov <- cluster.vcov(m1, ~ firmid + year)

coeftest(m1, m1$cluster1.vcov)
coeftest(m1, m1$cluster2.vcov)
```

```{r, results='asis'}
stargazer(m1, m1, 
          se = list(sqrt(diag(m1$cluster1.vcov)), 
                    sqrt(diag(m1$cluster2.vcov)) ), 
          header=F
) 
```

To pull off the SEs from the Variance Covaraince matrix, we need only to pull the squareroot of the diagonals of the VCOV. 

```{r}
m1$robust.se <-  sqrt(diag(m1$vcovHC))  # note that this is operating on the
                                        # object we already created
m1$cluster1.se <- sqrt(diag(m1$cluster1.vcov))
m1$cluster2.se <- sqrt(diag(m1$cluster2.vcov))

## for comparison, let's compute the OLS SEs
m1$ols.vcov <- vcovHC(m1, "const")
m1$ols.se   <- sqrt(diag(m1$ols.vcov))
``` 

With each of these esimated, we can produce a table that reports all the estimates. 

```{r, results='asis'}
stargazer(m1, m1, m1, m1,
          se = list(m1$ols.se, 
                    m1$robust.se,
                    m1$cluster1.se,
                    m1$cluster2.se),
          header=F)
```
